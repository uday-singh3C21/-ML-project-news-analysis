{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6f6569f-01e5-4355-a59f-f1d7c40612da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from newsapi import NewsApiClient\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from apscheduler.schedulers.background import BackgroundScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6177e67-3ac8-47e4-818b-a343bae3a1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: apscheduler in /opt/anaconda3/lib/python3.12/site-packages (3.11.0)\n",
      "Requirement already satisfied: tzlocal>=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from apscheduler) (5.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install apscheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bfc252c-af59-41c3-adda-ef34e05be959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Key for NewsAPI\n",
    "API_KEY = 'bcae1bfec35f4e78a3cdc7accdbffd86'\n",
    "newsapi = NewsApiClient(api_key=API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e3f4527-ef52-4998-9f8e-7d03d5cd1430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vectorizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load or initialize model and vectorizer\n",
    "try:\n",
    "    with open(\"sentiment_model.pkl\", \"rb\") as model_file:\n",
    "        model = pickle.load(model_file)\n",
    "    with open(\"vectorizer.pkl\", \"rb\") as vec_file:\n",
    "        vectorizer = pickle.load(vec_file)\n",
    "    print(\"Model and vectorizer loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No existing model found. Training from scratch.\")\n",
    "    model = LogisticRegression(max_iter=500)\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50ec42dd-e483-4138-96b9-7940349db46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch real-time news\n",
    "def fetch_real_time_news():\n",
    "    try:\n",
    "        articles = newsapi.get_everything(q='stock market', language='en', sort_by='publishedAt')\n",
    "        news_data = [{\n",
    "            'publishedAt': article['publishedAt'],\n",
    "            'text': f\"{article['title']} {article['description']} {article['content']}\"\n",
    "        } for article in articles['articles']]\n",
    "        return pd.DataFrame(news_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching news: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3eb444f4-c57b-4b18-9228-531e94f6ac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99feee73-f069-4e57-887f-b5c329718eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain model\n",
    "def retrain_model(new_data):\n",
    "    global model, vectorizer\n",
    "    try:\n",
    "        labeled_data = pd.read_csv(\"labeled_news_data.csv\")\n",
    "    except FileNotFoundError:\n",
    "        labeled_data = pd.DataFrame(columns=[\"clean_text\", \"sentiment\"])\n",
    "    \n",
    "    labeled_data = pd.concat([labeled_data, new_data], ignore_index=True)\n",
    "    \n",
    "    X = labeled_data['clean_text']\n",
    "    y = labeled_data['sentiment']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    model.fit(X_train_vec, y_train)\n",
    "    \n",
    "    with open(\"sentiment_model.pkl\", \"wb\") as model_file:\n",
    "        pickle.dump(model, model_file)\n",
    "    with open(\"vectorizer.pkl\", \"wb\") as vec_file:\n",
    "        pickle.dump(vectorizer, vec_file)\n",
    "    \n",
    "    print(\"Model retrained and updated successfully!\")\n",
    "    evaluate_model(X_test_vec, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b2ae706-457e-49b0-9f72-6dba7ca48c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "def evaluate_model(X_test_vec, y_test):\n",
    "    y_pred = model.predict(X_test_vec)\n",
    "    print(\"\\nModel Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap=\"Blues\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db1c2260-3f37-42d1-8682-57e83eb6b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated training pipeline\n",
    "def automate_training():\n",
    "    print(\"Fetching and processing new news data...\")\n",
    "    real_time_data = fetch_real_time_news()\n",
    "    if not real_time_data.empty:\n",
    "        real_time_data['clean_text'] = real_time_data['text'].apply(clean_text)\n",
    "        real_time_data['sentiment'] = model.predict(vectorizer.transform(real_time_data['clean_text']))\n",
    "        real_time_data[['clean_text', 'sentiment']].to_csv(\"labeled_news_data.csv\", mode='a', header=False, index=False)\n",
    "        retrain_model(real_time_data[['clean_text', 'sentiment']])\n",
    "    else:\n",
    "        print(\"No new data fetched.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e162571-961c-40f4-bad0-ae5f68c9d594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated model training started. Running every hour...\n",
      "Fetching and processing new news data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job \"automate_training (trigger: interval[1:00:00], next run at: 2025-02-10 20:02:53 IST)\" raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/apscheduler/executors/base.py\", line 131, in run_job\n",
      "    retval = job.func(*job.args, **job.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/wp/l1lbtpb57rl387801fcn64ch0000gn/T/ipykernel_9160/3781866117.py\", line 9, in automate_training\n",
      "    retrain_model(real_time_data[['clean_text', 'sentiment']])\n",
      "  File \"/var/folders/wp/l1lbtpb57rl387801fcn64ch0000gn/T/ipykernel_9160/2292364875.py\", line 5, in retrain_model\n",
      "    labeled_data = pd.read_csv(\"labeled_news_data.csv\")\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 626, in _read\n",
      "    return parser.read(nrows)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1923, in read\n",
      "    ) = self._engine.read(  # type: ignore[attr-defined]\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 234, in read\n",
      "    chunks = self._reader.read_low_memory(nrows)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"parsers.pyx\", line 838, in pandas._libs.parsers.TextReader.read_low_memory\n",
      "  File \"parsers.pyx\", line 905, in pandas._libs.parsers.TextReader._read_rows\n",
      "  File \"parsers.pyx\", line 874, in pandas._libs.parsers.TextReader._tokenize_rows\n",
      "  File \"parsers.pyx\", line 891, in pandas._libs.parsers.TextReader._check_tokenize_status\n",
      "  File \"parsers.pyx\", line 2061, in pandas._libs.parsers.raise_parser_error\n",
      "pandas.errors.ParserError: Error tokenizing data. C error: Expected 3 fields in line 684, saw 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scheduler to run training every hour\n",
    "scheduler = BackgroundScheduler()\n",
    "scheduler.add_job(automate_training, 'interval', hours=1)\n",
    "scheduler.start()\n",
    "\n",
    "print(\"Automated model training started. Running every hour...\")\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(60)\n",
    "except (KeyboardInterrupt, SystemExit):\n",
    "    scheduler.shutdown()\n",
    "    print(\"Scheduler stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a2f0d3-a381-4bdc-8220-58659cca2101",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load labeled dataset\n",
    "file_path = \"labeled_news_data.csv\"\n",
    "df = pd.read_csv(file_path, names=[\"publishedAt\", \"clean_text\", \"sentiment\"])  # Ensure correct column names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177385cf-5541-452a-9733-94df6875bce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Vectorize text data\n",
    "with open(\"vectorizer.pkl\", \"rb\") as vec_file:\n",
    "    vectorizer = pickle.load(vec_file)\n",
    "\n",
    "X = vectorizer.transform(df['clean_text'])  # Transform existing data\n",
    "y = df['sentiment']  # Labels (-1 = Negative, 0 = Neutral, 1 = Positive)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load the trained model\n",
    "with open(\"sentiment_model.pkl\", \"rb\") as model_file:\n",
    "    model = pickle.load(model_file)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print Accuracy and Classification Report\n",
    "print(\"\\n Model Accuracy:\", accuracy)\n",
    "print(\"\\n Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5ea7b9-f17b-4d8d-88af-1d81c39d82c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative\", \"Neutral\", \"Positive\"], yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eeeb43-3fe6-4c23-8c80-70638f2588ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
